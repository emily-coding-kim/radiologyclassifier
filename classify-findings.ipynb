{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8554e4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f2a36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Utilities\n",
    "def read_excel_file(file_path):\n",
    "    \"\"\"\n",
    "    Opens and reads input file Excel document\n",
    "    :param file_path: Path to the input Excel file\n",
    "    :return: DataFrame containing the contents of the Excel file\n",
    "    \"\"\"\n",
    "    # check if file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Input file {file_path} does not exist\")\n",
    "    \n",
    "    # check if file is an Excel file\n",
    "    if not file_path.endswith(('.xls', '.xlsx')):\n",
    "        raise ValueError(f\"Input file {file_path} is not an Excel file\")\n",
    "\n",
    "    # try reading the Excel file and saving it as a DataFrame\n",
    "    try:\n",
    "        df = pd.read_excel(file_path)\n",
    "\n",
    "    # raise a ValueError for any other exceptions that occur when reading the file \n",
    "    # with clearer error message\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error reading Excel file {file_path}: {e}\")\n",
    "    return df\n",
    "\n",
    "def gen_outputfilename(name_type):\n",
    "    \"\"\"Generates a timestamped output filename based on the name type\n",
    "    :param name_type: Type of the output file as string (e.g., 'ground_truth_scores')\n",
    "    :return: Timestamped output filename\n",
    "    \"\"\"\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_filename = f\"{name_type}_{timestamp}.json\"\n",
    "    return output_filename\n",
    "\n",
    "def write_json_file(json_data, output_filename):\n",
    "    \"\"\"\n",
    "    Writes the JSON data to a file\n",
    "    :param json_data: JSON data as a dictionary\n",
    "    :param output_filename: Path to the output JSON file\n",
    "    \"\"\"\n",
    "    if json_data is None:\n",
    "        print(\"No data to write to JSON file.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Write to JSON file\n",
    "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(json_data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"JSON data written to {output_filename}\")\n",
    "        return output_filename\n",
    "    except IOError as e:\n",
    "        print(f\"Error writing JSON file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e2b3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_cols_canine_thorax = [\n",
    "    'perihilar_infiltrate', 'pneumonia', 'bronchitis', 'interstitial', 'diseased_lungs',\n",
    "    'hypo_plastic_trachea', 'cardiomegaly', 'pulmonary_nodules', 'pleural_effusion', 'rtm',\n",
    "    'focal_caudodorsal_lung', 'focal_perihilar', 'pulmonary_hypoinflation', 'right_sided_cardiomegaly',\n",
    "    'pericardial_effusion', 'bronchiectasis', 'pulmonary_vessel_enlargement', 'left_sided_cardiomegaly',\n",
    "    'thoracic_lymphadenopathy', 'esophagitis', 'vhs_v2'\n",
    "]\n",
    "\n",
    "disease_cols_feline_thorax = [\n",
    "    'pulmonary_nodules', 'esophagitis', 'pneumonia', 'bronchitis', 'interstitial', 'diseased_lungs',\n",
    "    'hypo_plastic_trachea', 'cardiomegaly', 'pleural_effusion', 'perihilar_infiltrate', 'rtm',\n",
    "    'focal_caudodorsal_lung', 'right_sided_cardiomegaly', 'focal_perihilar', 'left_sided_cardiomegaly',\n",
    "    'bronchiectasis', 'pulmonary_vessel_enlargement', 'thoracic_lymphadenopathy', 'pulmonary_hypoinflation',\n",
    "    'pericardial_effusion', 'Fe_Alveolar'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f684975",
   "metadata": {},
   "source": [
    "## Make JSON file of ground truth scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d92d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cols_excel_file(excel_file_path, required_cols_excel, disease_columns):\n",
    "    \"\"\"    Verifies the structure of the Excel file and extracts relevant columns\n",
    "    :param excel_file_path: Path to the Excel file\n",
    "    :return: file path, DataFrame with relevant columns, and a list of disease columns\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the Excel file\n",
    "        df = read_excel_file(excel_file_path)\n",
    "        print(f\"Loaded Excel file with {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "        \n",
    "        # Check that all required columns exist\n",
    "        required_columns = required_cols_excel + disease_columns\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        \n",
    "        if missing_columns:\n",
    "            print(f\"Error: Missing columns: {missing_columns}\")\n",
    "            return None, None\n",
    "        \n",
    "        # Extract relevant columns and remove rows with no findings\n",
    "        extracted_df = df[required_columns].copy()\n",
    "        extracted_df = extracted_df.dropna(subset=['Findings'])  # Remove rows with no findings\n",
    "\n",
    "        return excel_file_path, extracted_df, disease_columns\n",
    "    \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "def json_setup(excel_file_path, len_extracted_df, disease_columns, model_name=None):\n",
    "    \"\"\"\n",
    "    Sets up a JSON structure from the extracted DataFrame\n",
    "    :param excel_file_path: Path to the reference Excel or json file\n",
    "    :param extracted_df: length of DataFrame with relevant columns\n",
    "    :param disease_columns: List of disease columns\n",
    "    :return: JSON structure as a dictionary\n",
    "    \"\"\"\n",
    "    json_data = {\n",
    "        \"source_file\": excel_file_path,\n",
    "        \"extraction_date\": datetime.datetime.now().isoformat(),\n",
    "        \"total_cases\": len_extracted_df,\n",
    "        \"disease_columns\": disease_columns,\n",
    "        \"model_name\": model_name if model_name else \"ground truth\",\n",
    "        \"cases\": []\n",
    "    }\n",
    "    return json_data\n",
    "\n",
    "def json_structure(excel_file_path, extracted_df, disease_columns):\n",
    "    \"\"\"\n",
    "    Creates a JSON structure from the extracted DataFrame\n",
    "    :param excel_file_path: Path to the Excel file\n",
    "    :param extracted_df: DataFrame with relevant columns\n",
    "    :param disease_columns: List of disease columns\n",
    "    :return: JSON structure as a dictionary\n",
    "    \"\"\"\n",
    "    json_data = json_setup(excel_file_path, len(extracted_df), disease_columns)\n",
    "    if json_data is None:\n",
    "        return None\n",
    "    # populate cases in df\n",
    "    # Add each case with CaseID, findings, and all disease classifications\n",
    "    for index, row in extracted_df.iterrows():\n",
    "        case_data = {\n",
    "            \"case_id\": str(row['CaseID']).strip(),\n",
    "            \"findings\": str(row['Findings']).strip()\n",
    "        }\n",
    "\n",
    "        # Add all disease classifications\n",
    "        for disease in disease_columns:\n",
    "            case_data[disease] = str(row[disease]).strip() if pd.notna(row[disease]) else \"Unknown\" # Positive/Negative value to str and removing whitespace (strip)\n",
    "\n",
    "        json_data[\"cases\"].append(case_data)\n",
    "\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0829f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to extract from ground truth scores excel file\n",
    "def extract_scores_from_excel(disease_columns, excel_file_path, output_filename=None):\n",
    "    \"\"\"\n",
    "    Extract CaseID, Findings, and all 21 disease classifications from Excel file and convert to JSON format\n",
    "    :param excel_file_path: Path to the Excel file\n",
    "    :param output_filename: Optional custom filename for output JSON\n",
    "    :return: DataFrame with all data, and output filename\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        excel_file_path, extracted_df, disease_columns = extract_cols_excel_file(excel_file_path, ['CaseID', 'Findings'], disease_columns)\n",
    "        \n",
    "        json_data = json_structure(excel_file_path, extracted_df, disease_columns)\n",
    "\n",
    "        # Generate output filename if not provided\n",
    "        if output_filename is None:\n",
    "            output_filename = gen_outputfilename(\"ground_truth_scores\")\n",
    "\n",
    "        write_json_file(json_data, output_filename)\n",
    "        \n",
    "        return extracted_df, output_filename\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Excel file: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cddc7d3",
   "metadata": {},
   "source": [
    "## AI Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5579927f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt for Google API\n",
    "def read_prompt_template(prompt_filename):\n",
    "    \"\"\"\n",
    "    Reads a prompt template from an .txt file\n",
    "    :param prompt_filename: The filename of the prompt template\n",
    "    :return: The content of the prompt template as a string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(prompt_filename, 'r') as file:\n",
    "            prompt_template = file.read()\n",
    "        return prompt_template\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Prompt template file '{prompt_filename}' not found.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def create_prompt(prompt_template, findings):\n",
    "    \"\"\"\n",
    "    Creates a prompt for the Google Generative AI API based on the findings\n",
    "    :param prompt_template: The prompt template as a string\n",
    "    :param findings: The radiology findings to include in the prompt\n",
    "    :return: The formatted prompt string\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        f\"{prompt_template}\\n\"\n",
    "        f\"Radiology Findings: {findings}\\n\"\n",
    "    )\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f1e664",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "my_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if not my_api_key:\n",
    "    print(\"Please set GOOGLE_API_KEY in your .env file\")\n",
    "    my_api_key = input(\"Enter your Google API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a90a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "def call_google_api(prompt, model='gemini-2.0-flash-lite', api_key=None):\n",
    "    \"\"\"\n",
    "    Calls the Google Generative AI API with the given prompt\n",
    "    :param prompt: The prompt to send to the Google Generative AI API\n",
    "    :param model: The model to use for the API call\n",
    "    :param api_key: The API key for authentication\n",
    "    :return: The response from the Google Generative AI API\n",
    "    \"\"\"\n",
    "    genai.configure(api_key=api_key)\n",
    "    model = genai.GenerativeModel(model)\n",
    "\n",
    "    generation_config = genai.types.GenerationConfig(\n",
    "        temperature=0.2  # adjust creativity (0.0-2.0)\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = model.generate_content(prompt, generation_config=generation_config)\n",
    "        response_text = response.text\n",
    "        \n",
    "        # Clean up markdown formatting if present\n",
    "        if response_text.startswith('```json'):\n",
    "            # Remove ```json from start and ``` from end\n",
    "            response_text = response_text.replace('```json', '').replace('```', '').strip()\n",
    "        elif response_text.startswith('```'):\n",
    "            # Remove ``` from start and end\n",
    "            response_text = response_text.replace('```', '').strip()\n",
    "        if 'summary' in response_text or '**' in response_text or '***' in response_text:\n",
    "            # Remove any markdown formatting or summary text\n",
    "            response_text = response_text.replace('**', '').replace('***', '').replace('in summary', '').strip()\n",
    "        # Remove any text or reading/trailing whitespace after the final closing brace\n",
    "        if response_text.endswith('}'):\n",
    "            response_text = response_text[:response_text.rfind('}') + 1].strip()\n",
    "        \n",
    "        return response_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"API Error: {e}\")\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb71a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract CaseID and Findings from ground truth json file\n",
    "def read_from_json(json_file_path, max_cases=None):\n",
    "    try:\n",
    "        # Read the ground truth JSON file\n",
    "        with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "            ground_truth_data = json.load(f)\n",
    "        \n",
    "        cases = ground_truth_data['cases']\n",
    "        if max_cases:\n",
    "            cases = cases[:max_cases]\n",
    "        return ground_truth_data, cases\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: JSON file {json_file_path} not found.\")\n",
    "        return None, None\n",
    "\n",
    "def call_with_prompt(prompt_template, findings, case_id, api_key):\n",
    "    \"\"\"\n",
    "    Calls the Google Generative AI API with a prompt\n",
    "    :param prompt_template: The prompt template as a string\n",
    "    :param findings: The radiology findings to include in the prompt\n",
    "    :return: The response from the API\n",
    "    \"\"\"\n",
    "    try:\n",
    "        prompt = create_prompt(prompt_template, findings)\n",
    "            \n",
    "        print(f\"📡 Making API call for case {case_id}\")\n",
    "        response = call_google_api(prompt, api_key=api_key)\n",
    "        print(f\"✅ API response received for case {case_id}\")\n",
    "        time.sleep(2)  # Sleep to avoid hitting API rate limits\n",
    "        return response\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error calling API for case {case_id}: {e}\")\n",
    "        response = None\n",
    "    \n",
    "def parse_response_as_json(response, case_id, disease_cols):\n",
    "    \"\"\"\n",
    "    Parses the API response as JSON\n",
    "    :param response: The response from the API\n",
    "    :param case_id: The CaseID for the current case\n",
    "    :param disease_cols: List of disease columns to include in the output\n",
    "    :return: Parsed JSON data or None if parsing fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ai_classifications = json.loads(response)\n",
    "                \n",
    "        # Create case data in ground truth format\n",
    "        ai_case_data = {\n",
    "            \"case_id\": case_id,\n",
    "        }\n",
    "\n",
    "        # Add AI predictions for each disease\n",
    "        for disease in disease_cols:\n",
    "            if disease in ai_classifications:\n",
    "                ai_case_data[disease] = ai_classifications[disease]\n",
    "            else:\n",
    "                ai_case_data[disease] = \"Unknown\"  # Fallback if AI didn't provide this disease\n",
    "\n",
    "        return ai_case_data\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error parsing API response for case {case_id}\")\n",
    "        return None\n",
    "\n",
    "def handle_parse_error(e, response, case_id, disease_cols):\n",
    "    \"\"\"\n",
    "    Handles parsing errors and returns a failed case entry\n",
    "    :param e: The exception raised during parsing\n",
    "    :param response: The API response that caused the error\n",
    "    :param case_id: The CaseID for the current case\n",
    "    :param disease_cols: List of disease columns to include in the output\n",
    "    :return: A dictionary with the case ID and an error message\n",
    "    \"\"\"\n",
    "    print(f\"  Warning: Failed to parse AI response for case {case_id}\")\n",
    "    print(f\"  Error: {e}\")\n",
    "    print(response)  # Print the raw response for debugging\n",
    "\n",
    "    # Create case with all Unknown values\n",
    "    ai_case_data = {\n",
    "       \"case_id\": case_id,\n",
    "    }\n",
    "\n",
    "    # Set all diseases to Unknown for failed cases\n",
    "    for disease in disease_cols:\n",
    "       ai_case_data[disease] = \"Unknown\"\n",
    "    \n",
    "    return ai_case_data\n",
    "\n",
    "    \n",
    "def summary(successful_predictions, failed_predictions, output_filename):\n",
    "    \"\"\"\n",
    "    Prints a summary of the AI processing results\n",
    "    :param successful_predictions: Number of successful predictions\n",
    "    :param failed_predictions: Number of failed predictions\n",
    "    :param output_filename: Name of the output file where predictions are saved\n",
    "    \"\"\"\n",
    "    print(f\"\\nAI processing complete!\")\n",
    "    print(f\"Successful predictions: {successful_predictions}\")\n",
    "    print(f\"Failed predictions: {failed_predictions}\")\n",
    "    print(f\"AI predictions saved to: {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61274b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ai_scorings(ground_truth_json_path, api_key, model_type=\"google\", max_cases=None, output_filename=None, prompt_filename=\"/Users/Emily/radiology-project/Prompts/canine_thorax_prompt.txt\"):\n",
    "    \"\"\"\n",
    "    Process cases and create AI predictions in the same format as ground truth scores\n",
    "    :param ground_truth_json_path: Path to your ground truth JSON file with cases\n",
    "    :param api_key: API key for AI service\n",
    "    :param model_type: \"google\" or \"openai\"  \n",
    "    :param max_cases: Optional limit on number of cases to process\n",
    "    :param output_filename: Optional custom filename for AI predictions\n",
    "    :return: AI predictions in ground truth format\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt_template = read_prompt_template(prompt_filename)\n",
    "\n",
    "    print(f\"ai_scorings function called at {datetime.datetime.now()}\")\n",
    "\n",
    "    try:\n",
    "        # Read the ground truth JSON file\n",
    "        with open(ground_truth_json_path, 'r', encoding='utf-8') as f:\n",
    "            ground_truth_data = json.load(f)\n",
    "        \n",
    "        cases = ground_truth_data['cases']\n",
    "        if max_cases:\n",
    "            cases = cases[:max_cases]\n",
    "\n",
    "        # Create AI predictions structure matching ground truth format\n",
    "        ai_predictions = json_setup(ground_truth_data['source_file'], len(cases), ground_truth_data['disease_columns'])\n",
    "        \n",
    "        successful_predictions = 0\n",
    "        failed_predictions = 0\n",
    "        \n",
    "        for i, case in enumerate(cases, 1):\n",
    "            case_id = case['case_id']\n",
    "            findings_text = case['findings']\n",
    "\n",
    "            print(f\"[AI_SCORINGS] Processing case {i}/{len(cases)} - CaseID: {case_id}\")\n",
    "\n",
    "            response = call_with_prompt(prompt_template, findings_text, case_id, api_key)\n",
    "            \n",
    "            \n",
    "            # Try to parse AI response as JSON\n",
    "            try:\n",
    "                ai_case_data = parse_response_as_json(response, case_id, ground_truth_data['disease_columns'])\n",
    "\n",
    "                if ai_case_data:\n",
    "                    successful_predictions += 1\n",
    "                    print(f\" Successfully processed case {case_id}\")\n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                ai_case_data = handle_parse_error(e, response, case_id, disease_cols=ground_truth_data['disease_columns'])\n",
    "                failed_predictions += 1\n",
    "            \n",
    "            ai_predictions[\"cases\"].append(ai_case_data)\n",
    "\n",
    "        # Generate output filename if not provided\n",
    "        if output_filename is None:\n",
    "            output_filename = gen_outputfilename(f\"ai_predictions_{model_type}\")\n",
    "        \n",
    "        # Write AI predictions to JSON file\n",
    "        write_json_file(ai_predictions, output_filename)\n",
    "        \n",
    "        summary(successful_predictions, failed_predictions, output_filename)\n",
    "        \n",
    "        return ai_predictions, output_filename\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing cases: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb4e0d5",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de692ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts number of positive and negative cases for each disease in a JSON file\n",
    "def count_pos_neg(json_file, disease):\n",
    "    \"\"\"\n",
    "    Counts the number of positive and negative cases in the given JSON file for a given disease.\n",
    "    :param json_file: JSON file containing case data, either ground truth or ai scores\n",
    "    :param disease: Disease column to count positives and negatives for\n",
    "    :return: Dictionary with counts of positive and negative cases\n",
    "    \"\"\"\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # extract disease_columns from the JSON data\n",
    "    if disease not in data['disease_columns']:\n",
    "        raise ValueError(f\"JSON file does not contain '{disease}' key\")\n",
    "\n",
    "    # Initialize counters\n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    "\n",
    "    # Count positives and negatives\n",
    "    for case in data['cases']:\n",
    "        if case[disease] == 'Positive':\n",
    "            pos_count += 1\n",
    "        elif case[disease] == 'Negative':\n",
    "            neg_count += 1\n",
    "\n",
    "    # Return counts\n",
    "    return {'Positive': pos_count, 'Negative': neg_count}\n",
    "# makes sure that all ai file cases are included in ground truth file\n",
    "def compare_check_files(ground_truth, ai_scores):\n",
    "    \"\"\"\n",
    "    Compares the ground truth and AI scores JSON files for consistency.\n",
    "    :param ground_truth: JSON file containing ground truth data\n",
    "    :param ai_scores: JSON file containing AI scores data\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # first, check if ai_scores has the same cases as ground_truth\n",
    "    if len(ground_truth['cases']) >= len(ai_scores['cases']):\n",
    "        raise ValueError(\"AI scores should have less than or equal to ground truth cases\")\n",
    "    # also make sure both have the same CaseIDs\n",
    "    ground_truth_ids = {case['case_id'] for case in ground_truth['cases']}\n",
    "    ai_scores_ids = {case['case_id'] for case in ai_scores['cases']}\n",
    "    if ground_truth_ids != ai_scores_ids:\n",
    "        raise ValueError(\"Ground truth and AI scores must have the same CaseIDs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a39d1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return Excel file, input file paths\n",
    "def create_confusion_matrix(ground_truth, ai_scores):\n",
    "    \"\"\"\n",
    "    Creates an Excel file containing confusion matrix from comparing ground truth and AI scores\n",
    "    :param ground_truth: JSON file containing ground truth data\n",
    "    :param ai_scores: JSON file containing AI scores data\n",
    "    :return: confusion matrix export as Excel file\n",
    "    \"\"\"\n",
    "\n",
    "    # compare the ground truth and ai_scores files\n",
    "    #compare_check_files(ground_truth, ai_scores)\n",
    "\n",
    "    # columns from Example Confusion Matrix\n",
    "    cols = [\n",
    "        'condition','tp_Positive','fn_Positive','tn_Positive','fp_Positive','Sensitivity','Specificity','Check',\n",
    "        'Positive Ground Truth','Negative Ground Truth','Ground Truth Check'\n",
    "    ]\n",
    "\n",
    "    # create df to hold confusion matrix data\n",
    "    df = pd.DataFrame(columns=cols)\n",
    "\n",
    "   # Load JSON files if they are file paths (strings)\n",
    "    if isinstance(ground_truth, str):\n",
    "        with open(ground_truth, 'r') as f:\n",
    "            ground_truth_data = json.load(f)\n",
    "    else:\n",
    "        ground_truth_data = ground_truth\n",
    "        \n",
    "    if isinstance(ai_scores, str):\n",
    "        with open(ai_scores, 'r') as f:\n",
    "            ai_scores_data = json.load(f)\n",
    "    else:\n",
    "        ai_scores_data = ai_scores\n",
    "    \n",
    "    # Get disease columns from the ground truth data\n",
    "    disease_columns = ground_truth_data['disease_columns']\n",
    "    \n",
    "    # Process each disease (not 'condition' - that field doesn't exist)\n",
    "    for disease in disease_columns:\n",
    "        # Count ground truth cases for this disease\n",
    "        gt_counts = count_pos_neg(ground_truth, disease)\n",
    "        \n",
    "        # Initialize confusion matrix counters\n",
    "        tp = fp = tn = fn = 0\n",
    "        \n",
    "        # Calculate confusion matrix values for this disease\n",
    "        for ai_case in ai_scores_data['cases']:\n",
    "            # Find corresponding ground truth case\n",
    "            gt_case = next((c for c in ground_truth_data['cases'] if c['case_id'] == ai_case['case_id']), None)\n",
    "            if not gt_case:\n",
    "                continue\n",
    "                \n",
    "            ai_pred = ai_case[disease]\n",
    "            gt_true = gt_case[disease]\n",
    "            \n",
    "            if ai_pred == 'Positive' and gt_true == 'Positive':\n",
    "                tp += 1\n",
    "            elif ai_pred == 'Positive' and gt_true == 'Negative':\n",
    "                fp += 1\n",
    "            elif ai_pred == 'Negative' and gt_true == 'Positive':\n",
    "                fn += 1\n",
    "            elif ai_pred == 'Negative' and gt_true == 'Negative':\n",
    "                tn += 1\n",
    "        \n",
    "        # Calculate metrics\n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        \n",
    "        # Create row for this disease\n",
    "        row = {\n",
    "            'condition': disease,\n",
    "            'tp_Positive': tp,\n",
    "            'fn_Positive': fn,\n",
    "            'tn_Positive': tn,\n",
    "            'fp_Positive': fp,\n",
    "            'Sensitivity': sensitivity,\n",
    "            'Specificity': specificity,\n",
    "            'Check': tp + fn + tn + fp,\n",
    "            'Positive Ground Truth': gt_counts['Positive'],\n",
    "            'Negative Ground Truth': gt_counts['Negative'],\n",
    "            'Ground Truth Check': gt_counts['Positive'] + gt_counts['Negative']\n",
    "        }\n",
    "        \n",
    "        # Use pd.concat instead of deprecated df.append\n",
    "        df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n",
    "    \n",
    "    # Export to Excel\n",
    "    df.to_excel('confusion_matrix.xlsx', index=False)\n",
    "    print(\"Confusion matrix created and saved as 'confusion_matrix.xlsx'\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cb368e",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7551f24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process your Excel file with disease classifications\n",
    "excel_file_path = \n",
    "complete_data, complete_json_filename = extract_scores_from_excel(disease_cols_feline_thorax, excel_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2f08c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_scores, ai_json_filename = ai_scorings(\n",
    "    ground_truth_json_path=complete_json_filename,\n",
    "    api_key=my_api_key,\n",
    "    model_type=\"google\",  # or \"openai\"\n",
    "    max_cases=None,  # Process all cases\n",
    "    output_filename=None  # Auto-generate filename\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0589fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_confusion_matrix(ground_truth=complete_json_filename, ai_scores=ai_json_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
